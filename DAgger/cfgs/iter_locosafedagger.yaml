defaults:
  - _self_
  - override hydra/launcher: submitit_local

# -------- Pipeline Control --------
n_iteration: 4

# -------- Paths --------
output_root: ./data/multigoal-locosafedagger
run_dir: ${output_root}/${now:%b_%d_%Y_%H_%M_%S}

policy_path: null
pretrained_policy_path: "" 
database_path: "" 
pretrain_dataset_path: /home/atari/workspace/DAgger/example/data/behavior_cloning/trot/May_06_2025_11_21_24/dataset/database_final.hdf5
initial_policy_path: /home/atari/workspace/DAgger/example/data/behavior_cloning/trot/May_06_2025_11_21_24/network/policy_final.pth
reference_mpc_path: /home/atari/workspace/Behavior_Cloning/examples/data/behavior_cloning/trot/Apr_16_2025_13_02_09/dataset/experiment/traj_nominal_04_16_2025_13_02_15.npz

# -------- General --------
robot_name: "go2"
goal_type: "vc"
action_type: "pd_target"
gaits: ["trot"]
n_state: 44
n_action: 12
goal_horizon: 1
kp: 20
kd: 1.5

# -------- Data Collection --------
# speed limits
vx_des_min: 0.0
vx_des_max: 0.3
vy_des_min: 0.0
vy_des_max: 0.3
w_des_min: 0.0
w_des_max: 0.0

# simulation related
episode_length: 2000
sim_dt: 0.001
sim_time: 10
start_time: 0.0
visualize: True
save_data: True
record_video: False
interactive: False
v_des: [0.15, 0.0, 0.0]
initial_control_mode: "policy"

# -------- Policy Training --------
normalize_policy_input: True
batch_size: 256
learning_rate: 0.001
n_epoch: 15
n_train_frac: 0.9
num_hidden_layer: 3
hidden_dim: 512

# -------- Data Management --------
database_size: 10000000
suffix: ""

# -------- Hydra --------
hydra:
  job:
    chdir: False
  run:
    dir: ${run_dir}/hydra_logs
  sweep:
    dir: ${output_root}/${now:%Y_%m_%d}_${now:%H_%M_%S}
    subdir: ${hydra.job.num}
  launcher:
    timeout_min: 4300
    cpus_per_task: 10
    gpus_per_node: 1
    tasks_per_node: 1
    mem_gb: 160
    nodes: 1
    submitit_folder: ./exp/${now:%Y.%m.%d}/${now:%H%M%S}_${gaits}/.slurm
